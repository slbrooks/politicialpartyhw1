{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb5a852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b5b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9949ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc17827",
   "metadata": {},
   "source": [
    "# HW1 - Classification models in sklearn\n",
    "\n",
    "You'll be building a few classifier models and using some of the tech tools we learned about in Modules 1 and 2. \n",
    "\n",
    "## The Data\n",
    "\n",
    "The data is a relatively small and simple dataset of taxpayer data. I got it from:\n",
    "\n",
    "https://www.kaggle.com/dmaillie/sample-us-taxpayer-dataset\n",
    "\n",
    "As you'll see if you visit that page, this dataset was used in a series of YouTube tutorials on using R to build random forest models. \n",
    "\n",
    "I read it into a pandas dataframe and used `info()` to get:\n",
    "\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1004 entries, 0 to 1003\n",
    "Data columns (total 10 columns):\n",
    " #   Column          Non-Null Count  Dtype \n",
    "---  ------          --------------  ----- \n",
    " 0   HHI             1004 non-null   int64 \n",
    " 1   HHDL            1004 non-null   int64 \n",
    " 2   Married         1004 non-null   int64 \n",
    " 3   CollegGrads     1004 non-null   int64 \n",
    " 4   AHHAge          1004 non-null   int64 \n",
    " 5   Cars            1004 non-null   int64 \n",
    " 6   Filed_2017      1004 non-null   int64 \n",
    " 7   Filed_2016      1004 non-null   int64 \n",
    " 8   Filed_2015      1004 non-null   int64 \n",
    " 9   PoliticalParty  1004 non-null   object\n",
    "dtypes: int64(9), object(1)\n",
    "memory usage: 78.6+ KB\n",
    "```\n",
    "\n",
    "Some information about the fields:\n",
    "\n",
    "* `HHI` - household income\n",
    "* `HHDL` - household debt level\n",
    "* `Married` - categorical with a few levels\n",
    "* `CollegGrads` - number of college grads in the household\n",
    "* `AHHAge` - average age of people in the household\n",
    "* `Cars` - number of cars in the household\n",
    "* `Filed_2017` - 1 means they filed a tax return with the IRS for 2017\n",
    "* `Filed_2016` - 1 means they filed a tax return with the IRS for 2016\n",
    "* `Filed_2015` - 1 means they filed a tax return with the IRS for 2015\n",
    "* `PoliticalParty` - categorical with 3 levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabbbf5",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Our overall goal is to build classifier models to predict `PoliticalParty` using the the other variables. You must use sklearn Pipelines that contain your preprocessing steps and your model estimation step.\n",
    "\n",
    "You can do your work in a Jupyter Notebook(s) or in a Python script(s) (i.e. a ``.py`` file) or both. It's up to you.\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Start by creating a new project folder structure with the cookiecutter-datascience-simple template that I covered in Module 1. Put the data file into its appropriate folder and put this notebook in the main project folder. Any additional notebooks and/or Python files you end up creating should go in the main project folder. \n",
    "\n",
    "### Task 2\n",
    "\n",
    "Put your new project folder under version control using git. You should **NOT** track the data file. You must track any notebooks, Python scripts or additional text files you end up creating.\n",
    "\n",
    "### Task 3\n",
    "\n",
    "Build at least one logistic regression model (with regularization) and one random forest model to predict `PoliticalParty`. Yes, this is very similar to what we did for the Pump it Up project in Module 2. Some detailed requirements and additional information:\n",
    "\n",
    "* I suggest you start by reading the csv file into a pandas dataframe. My dataframe is called ``tax_df``.\n",
    "* Then start with some basic EDA. You can certainly use automated tools such as pandas-profiling or SweetViz as I showed in the class notes. Remember, when you run either of those, you **must** have your notebook open in the classic Jupyter Notebook interface (and **NOT** in Jupyter Lab). Once you've created the EDA reports you can close your notebook and reopen in Jupyter Lab if you wish. As we've seen, the reports get created as HTML documents. These should go in your output folder within your project.\n",
    "* Since we are using regularization, all of the numeric variables should be rescaled using the `StandardScaler` - be careful, just because a variable has a numeric datatype in the pandas dataframe, it does not mean that it's necessarily a numeric variable in the context of the classification models. Think about each column and look at your EDA reports and decide whether or not it's truly numeric or needs to be treated as categorical data in the models.\n",
    "* For any variables that you decide should be treated as categorical in your models, use the `OneHotEncoder` on them in the preprocessing stage.\n",
    "* Even though our target variable, `PoliticalParty`, is categorical, you do **NOT** need to do any preprocessing on it. As I mentioned in our class notes, scikit-learn will automatically detect that and will do any encoding needed on its own (it uses the `LabelEncoder`).\n",
    "* I broke up the ``tax_df`` into two separate dataframes that I called ``X`` and ``y``, to use in the models. Here's my code for that:\n",
    "\n",
    "```\n",
    "X = tax_df.iloc[:, 0:9]\n",
    "y = tax_df.iloc[:, 9]\n",
    "```\n",
    "\n",
    "* Please use the following code for your data partitioning so that we all end up with the same training and test split:\n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "```\n",
    "\n",
    "* For each model you fit, you should compute its ``score`` and create a confusion matrix on both train and test data. I did all of this repeatedly in the class notes.\n",
    "* For each model (the logistic regression model and the random forest) you should make some summary comments about how well the model fits and predicts and if there is evidence of overfitting. \n",
    "\n",
    "**IMPORTANT** You always should put summary comments in a markdown cell. Do **NOT** write them as comments in a code cell. The whole point of Jupyter notebooks is to be able to mix markdown cells with code cells. If you choose to do all of your Python work in a ``.py`` file(s), then simple create a Jupyter notebook in which you include your summary comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557525ae",
   "metadata": {},
   "source": [
    "#### Read in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e96c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df = pd.read_csv(\"./data/TaxInfo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381a671",
   "metadata": {},
   "source": [
    "#### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6998122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1004 entries, 0 to 1003\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   HHI             1004 non-null   int64 \n",
      " 1   HHDL            1004 non-null   int64 \n",
      " 2   Married         1004 non-null   int64 \n",
      " 3   CollegGrads     1004 non-null   int64 \n",
      " 4   AHHAge          1004 non-null   int64 \n",
      " 5   Cars            1004 non-null   int64 \n",
      " 6   Filed_2017      1004 non-null   int64 \n",
      " 7   Filed_2016      1004 non-null   int64 \n",
      " 8   Filed_2015      1004 non-null   int64 \n",
      " 9   PoliticalParty  1004 non-null   object\n",
      "dtypes: int64(9), object(1)\n",
      "memory usage: 78.6+ KB\n"
     ]
    }
   ],
   "source": [
    "tax_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072490a",
   "metadata": {},
   "source": [
    "#### Panda Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8c05dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc0805eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile = ProfileReport(tax_df, title=\"Pandas Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b2620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile.to_file(\"output/hw1_pandas_profiling_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee274fe8",
   "metadata": {},
   "source": [
    "#### Sweetviz Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "698d07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f59c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140c9d0cefe047f68184b92dddb0d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                             |                                             | [  0%]   00:00 ->…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = sweetviz.analyze(tax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e3e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report output/hw1_sweetviz_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "report.show_html(\"output/hw1_sweetviz_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16c8ad-b138-4f8c-8799-9e1f2da0f3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80debddf-605b-4594-aaf5-7687a84cac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HHI',\n",
       " 'HHDL',\n",
       " 'Married',\n",
       " 'CollegGrads',\n",
       " 'AHHAge',\n",
       " 'Cars',\n",
       " 'Filed_2017',\n",
       " 'Filed_2016',\n",
       " 'Filed_2015']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_df.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cdb22c9-a547-4d2f-b1f6-abfac8944e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoliticalParty\n",
       "Democrat       336\n",
       "Independent    337\n",
       "Republican     331\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_df.groupby(['PoliticalParty']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb429e5a-99b3-4b0e-89dc-3d6f39991e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Independent    0.335657\n",
       "Democrat       0.334661\n",
       "Republican     0.329681\n",
       "Name: PoliticalParty, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_df['PoliticalParty'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4abd0-ca29-4254-b30b-a54800a06c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2dc5d8-72f8-4cee-8aa6-cbdf098006a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4fe4a-2222-4d1c-a94a-78292e1db934",
   "metadata": {},
   "source": [
    "#### Creating X and Y Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03073201-a97a-4d4b-9f1d-82a2588ecbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tax_df.iloc[:, 0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72701c-a9ea-4649-be19-2a2c57fac199",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tax_df.iloc[:, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06b104-6a37-4e0d-84ef-7d340d550165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a2763-7c45-40e9-b31f-5e09eb3e75ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e850a9-7597-445c-b6a0-efd18535c85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30deeca4-4020-406f-86a5-7587cceffb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89c9d2-43c8-4854-8ee7-a88b53c52837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca73e7-8e50-4777-9107-588d237cf7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c829f6e-972e-4b9e-a9c0-4b3e450144e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a026c076",
   "metadata": {},
   "source": [
    "## Optional Hacker Extra tasks\n",
    "I always like to include some extra credit tasks for those who want to push themselves a little further. For this problem, consider doing one or more of the following:\n",
    "\n",
    "* Try out the Histogram based Gradient Boosting Classifier shown in the optional materials at the end of Module 2. Compare its performance to logistic regression and the random forest.\n",
    "* Create a second set of models in which you treat ``Filed_2017`` as a binary target variable and use ``PoliticalParty`` as a categorical feature variable. Is it any easier to predict ``Filed_2017`` than it was to predict ``PoliticalParty``?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32e527",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "You should simply compress your entire project folder as either a zip file or a tar.gz file (do **NOT** ever use WinRAR to create rar files in this class). Note that when you do this, your \"hidden\" ``.git`` folder will get included. So, I'll be able to tell that you put the project under version control and I'll be able to look at your project folder structure. Before compressing the project folder to submit it:\n",
    "\n",
    "* make sure all of your notebooks and .py files are in the main project folder and have good filenames,\n",
    "* make sure you've committed all of your changes (git),\n",
    "* upload your compressed folder in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0ad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
